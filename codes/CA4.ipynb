{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI CA4\n",
    "<br>\n",
    "sajjad pakdaman savoji\n",
    "<br>\n",
    "810195517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"><b> Question 1 </b></font>  : Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from scipy.stats import mode\n",
    "import operator\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data , split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "train , test = split(data , test_size = 53/303 , random_state = 12347 , shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train.target\n",
    "train_feature = train.drop(['target'] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = test.target\n",
    "test_feature = test.drop(['target'] , axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decision Tree model\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.<br>\n",
    "https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(train_feature , train_label);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT acc on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7924528301886793\n"
     ]
    }
   ],
   "source": [
    "pred = dt.predict(test_feature)\n",
    "print(acc(pred , test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT acc on train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "pred = dt.predict(train_feature)\n",
    "print(acc(pred , train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## total performance of decision tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7529245283018867\n",
      "0.03127739098613014\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(1000):\n",
    "    data = pd.read_csv('data.csv')\n",
    "    train , test = split(data , test_size = 53/303 , random_state = 12347 , shuffle = True)\n",
    "\n",
    "    train_label = train.target\n",
    "    train_feature = train.drop(['target'] , axis = 1)\n",
    "\n",
    "    test_label = test.target\n",
    "    test_feature = test.drop(['target'] , axis = 1)\n",
    "\n",
    "    # decision Tree model\n",
    "\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    dt.fit(train_feature , train_label);\n",
    "\n",
    "    ## acc on test data \n",
    "\n",
    "    pred = dt.predict(test_feature)\n",
    "    res.append(acc(pred , test_label))\n",
    "    \n",
    "res = np.array(res)\n",
    "print(res.mean())\n",
    "print(res.std())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"><b> Question 2 </b></font>  : Steps to impliment Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'green'>2.1 </font> make 5 samples of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLF = 5\n",
    "SAMPLE_SZ  = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"green\">2.2</font> bagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bagging():\n",
    "    def __init__(self, model = tree.DecisionTreeClassifier , n_clf = 5):\n",
    "        self.models = [model() for _ in range(n_clf)]\n",
    "        self.N_CLF = n_clf\n",
    "        self.samples_features = []\n",
    "        self.samples_labels = []\n",
    "        \n",
    "    def fit(self , features , labels , sample_sz = 150):\n",
    "        for i in range(self.N_CLF):\n",
    "            si = features.sample(150 , replace = True)\n",
    "            li = labels.loc[si.index]\n",
    "\n",
    "            self.samples_features.append(si)\n",
    "            self.samples_labels.append(li)\n",
    "            \n",
    "        for i in range(self.N_CLF):\n",
    "            self.models[i].fit(self.samples_features[i] , self.samples_labels[i])\n",
    "            \n",
    "    def predict(self , features):\n",
    "        pred = []\n",
    "        for model in self.models:\n",
    "            pred.append(np.array(model.predict(features)))\n",
    "        \n",
    "        pred = np.array(pred)\n",
    "        return mode(pred , axis = 0)[0][0]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8113207547169812\n"
     ]
    }
   ],
   "source": [
    "ensemble = Bagging()\n",
    "ensemble.fit(train_feature , train_label)\n",
    "pred = ensemble.predict(test_feature)\n",
    "print(acc(pred , test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging preformance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8100754716981132\n",
      "0.04044802491876607\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "for i in range(1000):\n",
    "    ensemble = Bagging()\n",
    "    ensemble.fit(train_feature , train_label)\n",
    "    pred = ensemble.predict(test_feature)\n",
    "    accuracy.append(acc(pred , test_label))\n",
    "    \n",
    "accuracy = np.array(accuracy)\n",
    "print(accuracy.mean())\n",
    "print(accuracy.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'green'>2.3</font> backward elimination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "model = tree.DecisionTreeClassifier\n",
    "for feature in train_feature.columns :\n",
    "    res_f = []\n",
    "    for i in range(500):\n",
    "        train_feature_i = train_feature.drop(feature , axis = 1)\n",
    "        test_feature_i = test_feature.drop(feature , axis = 1)\n",
    "\n",
    "        mi = model()\n",
    "        mi.fit(train_feature_i , train_label)\n",
    "        predi = mi.predict(test_feature_i)\n",
    "        res_f.append(acc(predi , test_label))\n",
    "    res_f = np.array(res_f)\n",
    "    res[feature] = res_f.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 0.8008679245283019, 'sex': 0.7969056603773585, 'cp': 0.8242264150943396, 'trestbps': 0.7983773584905661, 'chol': 0.725433962264151, 'fbs': 0.8073584905660377, 'restecg': 0.7425660377358491, 'thalach': 0.7447169811320755, 'exang': 0.732943396226415, 'oldpeak': 0.7233584905660377, 'slope': 0.7481509433962263, 'ca': 0.7970188679245285, 'thal': 0.7310188679245283}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cp'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(res.items(), key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"green\">2.4</font> random selection of features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['oldpeak', 'exang', 'slope', 'age', 'sex'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = np.array(train_feature.columns)\n",
    "selected = np.random.choice(columns , size = 5 , replace = False)\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_selected = train_feature[selected]\n",
    "test_feature_selected = test_feature[selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7547169811320755\n"
     ]
    }
   ],
   "source": [
    "model = tree.DecisionTreeClassifier\n",
    "mi = model()\n",
    "mi.fit(train_feature_selected , train_label)\n",
    "pred = mi.predict(test_feature_selected)\n",
    "print(acc(pred , test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random selection performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7125534591194969\n",
      "0.07143346943939437\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(3000):\n",
    "    #select some random features\n",
    "    columns = np.array(train_feature.columns)\n",
    "    selected = np.random.choice(columns , size = 5 , replace = False)\n",
    "    \n",
    "    #only take those features that was selected\n",
    "    train_feature_selected = train_feature[selected]\n",
    "    test_feature_selected = test_feature[selected]\n",
    "    \n",
    "    #choose some model and train it\n",
    "    model = tree.DecisionTreeClassifier\n",
    "    mi = model()\n",
    "    mi.fit(train_feature_selected , train_label)\n",
    "    \n",
    "    #predict using model and record the acc\n",
    "    pred = mi.predict(test_feature_selected)\n",
    "    res.append(acc(pred , test_label))\n",
    "    \n",
    "res = np.array(res)\n",
    "print(res.mean())\n",
    "print(res.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = \"green\">2.5 </font> Random forest implimentation\n",
    "https://towardsdatascience.com/understanding-random-forest-58381e0602d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self , n , model = tree.DecisionTreeClassifier):\n",
    "        self.n = n\n",
    "        self.model = model\n",
    "        self.models = [self.model() for _ in range(self.n)]\n",
    "        self.samples_features = []\n",
    "        self.samples_labels = []\n",
    "        \n",
    "    def fit(self , features , labels):\n",
    "        # this part is for randomness in features\n",
    "        all_features = features.columns\n",
    "        max_feature = len(all_features)\n",
    "        num_features = [np.random.randint(low = 1 , high = max_feature + 1) \n",
    "                        for _ in range (self.n)]\n",
    "        self.selected_features = [np.random.choice(all_features , size = num_features[i] \n",
    "                                              ,replace = False) for i in range (self.n)]\n",
    "        \n",
    "        # this part is for bagging\n",
    "        for i in range(self.n):\n",
    "            si = features.sample(int(0.3*len(features)) , replace = True)\n",
    "            li = labels.loc[si.index]\n",
    "\n",
    "            # chooze only selected feaures\n",
    "            si = si[self.selected_features[i]]\n",
    "            \n",
    "            \n",
    "            self.samples_features.append(si)\n",
    "            self.samples_labels.append(li)\n",
    "            \n",
    "        # then fit the models\n",
    "        for i in range(self.n):\n",
    "            self.models[i].fit(self.samples_features[i] , self.samples_labels[i])\n",
    "            \n",
    "    def predict(self , features):\n",
    "        pred = []\n",
    "        for i , model in enumerate(self.models):\n",
    "            features_i = features[self.selected_features[i]]\n",
    "            pred.append(np.array(model.predict(features_i)))\n",
    "        \n",
    "        pred = np.array(pred)\n",
    "        return mode(pred , axis = 0)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForest(10) \n",
    "forest.fit(train_feature , train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8490566037735849\n"
     ]
    }
   ],
   "source": [
    "pred = forest.predict(test_feature)\n",
    "print(acc(pred , test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  random forest performance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8549056603773585\n",
      "0.02807392468856127\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(100):\n",
    "    forest = RandomForest(50) \n",
    "    forest.fit(train_feature , train_label)\n",
    "    pred = forest.predict(test_feature)\n",
    "    res.append(acc(pred , test_label))\n",
    "    \n",
    "res = np.array(res)\n",
    "print(res.mean())\n",
    "print(res.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"><b> Report </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"green\"> <b>Question 1: </b></font> Bootstraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, each tree in a random forest learns from a random sample of the data points. The samples are drawn with replacement, known as bootstrapping, which means that some samples will be used multiple times in a single tree.\n",
    "<br><br>\n",
    "\n",
    "bootstraping increases standard deviation(and variance) of accuracy of model because additional randomness is forced to model resulting less deterministic behaviour\n",
    "<br><br>\n",
    "\n",
    "boostraping adds bias error thus improoving variance error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"green\"> <b>Question 2: </b></font> Overfiting and Bagging\n",
    "Over-fitting is the phenomenon in which the learning system tightly fits the given training data so much that it would be inaccurate in predicting the outcomes of the untrained data. In decision trees, over-fitting occurs when the tree is designed so as to perfectly fit all samples in the training data set.<br><br>\n",
    "In decision tree , if depth of tree is not limited and entropy gain is used as a discrimianant refrence for feature selection , tree will have a 100% accuracy on train data , in contrast performance of model would not be as perfect.\n",
    "By implimenting Baggin additional random ness is forced to model resulting to better generalization and worse accuracy on train data itself.This additive randomness is the classic bias-variance trade off struggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"green\"> <b>Question 3: </b></font> Random Forest and Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental difference is that in Random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node.<br>\n",
    "In a more widesense comparison Random Forest uses Bagging to overcome overfiting in decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction (see figure below).\n",
    "<br>\n",
    "<img src = \"https://miro.medium.com/max/526/1*VHDtVaDPNepRglIAv72BFg.jpeg\"><br>\n",
    "The fundamental concept behind random forest is a simple but powerful one — the wisdom of crowds. In data science speak, the reason that the random forest model works so well is:\n",
    "<b> A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low correlation between models is the key. Just like how investments with low correlations (like stocks and bonds) come together to form a portfolio that is greater than the sum of its parts, uncorrelated models can produce ensemble predictions that are more accurate than any of the individual predictions. The reason for this wonderful effect is that the trees protect each other from their individual errors (as long as they don’t constantly all err in the same direction). While some trees may be wrong, many other trees will be right, so as a group the trees are able to move in the correct direction. So the prerequisites for random forest to perform well are:\n",
    "<br><br>\n",
    "1- There needs to be some actual signal in our features so that models built using those features do better than random guessing.\n",
    "<br><br>\n",
    "2- The predictions (and therefore the errors) made by the individual trees need to have low correlations with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = \"green\"> <b>Question 4: </b></font> Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT accuracy (avg)      : 0.7529 <br>\n",
    "Bagging accuracy (avg) : 0.8100 <br>\n",
    "Random forest (avg)    : 0.8549 <br><br>\n",
    "\n",
    "<font color = \"blue\"><b>A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.</b></font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
